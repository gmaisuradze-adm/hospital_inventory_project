"""
Evaluation Utilities for Hospital Inventory AI Module

This module provides comprehensive evaluation metrics and utilities for:
- Forecasting model performance evaluation
- Inventory optimization effectiveness
- Business impact assessment
- Model comparison and selection
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import logging
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

@dataclass
class EvaluationMetrics:
    """Container for evaluation metrics"""
    mae: float
    mse: float
    rmse: float
    mape: float
    smape: float
    r2: float
    bias: float
    accuracy: float
    directional_accuracy: float

@dataclass
class BusinessMetrics:
    """Container for business impact metrics"""
    cost_reduction: float
    service_level_improvement: float
    inventory_turnover: float
    stockout_reduction: float
    carrying_cost_savings: float
    total_roi: float

class ForecastEvaluator:
    """
    Comprehensive evaluation suite for demand forecasting models
    """
    
    def __init__(self, tolerance: float = 0.1):
        """
        Initialize the evaluator
        
        Args:
            tolerance: Tolerance level for accuracy calculations
        """
        self.tolerance = tolerance
    
    def calculate_metrics(self, 
                         actual: Union[pd.Series, np.ndarray], 
                         predicted: Union[pd.Series, np.ndarray]) -> EvaluationMetrics:
        """
        Calculate comprehensive forecasting metrics
        
        Args:
            actual: Actual demand values
            predicted: Predicted demand values
            
        Returns:
            EvaluationMetrics object with all calculated metrics
        """
        # Convert to numpy arrays
        actual = np.array(actual)
        predicted = np.array(predicted)
        
        # Remove any NaN values
        mask = ~(np.isnan(actual) | np.isnan(predicted))
        actual = actual[mask]
        predicted = predicted[mask]
        
        if len(actual) == 0:
            logger.warning("No valid data points for evaluation")
            return self._empty_metrics()
        
        # Basic metrics
        mae = mean_absolute_error(actual, predicted)
        mse = mean_squared_error(actual, predicted)
        rmse = np.sqrt(mse)
        
        # Percentage errors
        mape = self._calculate_mape(actual, predicted)
        smape = self._calculate_smape(actual, predicted)
        
        # R-squared
        r2 = r2_score(actual, predicted)
        
        # Bias
        bias = np.mean(predicted - actual)
        
        # Accuracy (within tolerance)
        accuracy = self._calculate_accuracy(actual, predicted)
        
        # Directional accuracy
        directional_accuracy = self._calculate_directional_accuracy(actual, predicted)
        
        return EvaluationMetrics(
            mae=mae,
            mse=mse,
            rmse=rmse,
            mape=mape,
            smape=smape,
            r2=r2,
            bias=bias,
            accuracy=accuracy,
            directional_accuracy=directional_accuracy
        )
    
    def _calculate_mape(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """Calculate Mean Absolute Percentage Error"""
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # Avoid division by zero
            actual_nonzero = actual[actual != 0]
            predicted_nonzero = predicted[actual != 0]
            
            if len(actual_nonzero) == 0:
                return 0.0
                
            return np.mean(np.abs((actual_nonzero - predicted_nonzero) / actual_nonzero)) * 100
    
    def _calculate_smape(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """Calculate Symmetric Mean Absolute Percentage Error"""
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            denominator = (np.abs(actual) + np.abs(predicted)) / 2
            
            # Avoid division by zero
            mask = denominator != 0
            if np.sum(mask) == 0:
                return 0.0
                
            return np.mean(np.abs(actual[mask] - predicted[mask]) / denominator[mask]) * 100
    
    def _calculate_accuracy(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """Calculate accuracy within tolerance"""
        errors = np.abs(actual - predicted)
        tolerance_values = np.maximum(np.abs(actual) * self.tolerance, 1.0)
        within_tolerance = errors <= tolerance_values
        return np.mean(within_tolerance) * 100
    
    def _calculate_directional_accuracy(self, actual: np.ndarray, predicted: np.ndarray) -> float:
        """Calculate directional accuracy (trend prediction)"""
        if len(actual) < 2:
            return 0.0
            
        actual_diff = np.diff(actual)
        predicted_diff = np.diff(predicted)
        
        # Check if both predict the same direction
        same_direction = np.sign(actual_diff) == np.sign(predicted_diff)
        return np.mean(same_direction) * 100
    
    def _empty_metrics(self) -> EvaluationMetrics:
        """Return empty metrics for error cases"""
        return EvaluationMetrics(
            mae=0.0, mse=0.0, rmse=0.0, mape=0.0, smape=0.0,
            r2=0.0, bias=0.0, accuracy=0.0, directional_accuracy=0.0
        )
    
    def compare_models(self, 
                      actual: np.ndarray,
                      predictions: Dict[str, np.ndarray]) -> pd.DataFrame:
        """
        Compare multiple forecasting models
        
        Args:
            actual: Actual demand values
            predictions: Dictionary of model predictions
            
        Returns:
            DataFrame with comparison results
        """
        results = []
        
        for model_name, predicted in predictions.items():
            metrics = self.calculate_metrics(actual, predicted)
            results.append({
                'Model': model_name,
                'MAE': metrics.mae,
                'RMSE': metrics.rmse,
                'MAPE': metrics.mape,
                'RÂ²': metrics.r2,
                'Accuracy': metrics.accuracy,
                'Directional_Accuracy': metrics.directional_accuracy
            })
        
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('MAE')  # Sort by MAE
        return results_df
    
    def calculate_forecast_intervals(self,
                                   predictions: np.ndarray,
                                   residuals: np.ndarray,
                                   confidence_level: float = 0.95) -> Tuple[np.ndarray, np.ndarray]:
        """
        Calculate forecast confidence intervals
        
        Args:
            predictions: Point forecasts
            residuals: Model residuals
            confidence_level: Confidence level (0-1)
            
        Returns:
            Tuple of (lower_bound, upper_bound)
        """
        from scipy import stats
        
        # Calculate standard error
        std_error = np.std(residuals)
        
        # Calculate critical value
        alpha = 1 - confidence_level
        critical_value = stats.norm.ppf(1 - alpha/2)
        
        # Calculate intervals
        margin_of_error = critical_value * std_error
        lower_bound = predictions - margin_of_error
        upper_bound = predictions + margin_of_error
        
        return lower_bound, upper_bound

class InventoryEvaluator:
    """
    Evaluation suite for inventory optimization performance
    """
    
    def __init__(self):
        self.baseline_metrics = {}
    
    def evaluate_optimization_performance(self,
                                        current_inventory: pd.DataFrame,
                                        optimized_inventory: pd.DataFrame,
                                        demand_data: pd.DataFrame,
                                        cost_parameters: Dict[str, float]) -> BusinessMetrics:
        """
        Evaluate the business impact of inventory optimization
        
        Args:
            current_inventory: Current inventory state
            optimized_inventory: Optimized inventory recommendations
            demand_data: Historical demand data
            cost_parameters: Cost structure parameters
            
        Returns:
            BusinessMetrics with business impact assessment
        """
        # Calculate current state metrics
        current_metrics = self._calculate_inventory_metrics(
            current_inventory, demand_data, cost_parameters
        )
        
        # Calculate optimized state metrics
        optimized_metrics = self._calculate_inventory_metrics(
            optimized_inventory, demand_data, cost_parameters
        )
        
        # Calculate improvements
        cost_reduction = (current_metrics['total_cost'] - optimized_metrics['total_cost']) / current_metrics['total_cost'] * 100
        service_improvement = optimized_metrics['service_level'] - current_metrics['service_level']
        turnover_improvement = optimized_metrics['inventory_turnover'] - current_metrics['inventory_turnover']
        stockout_reduction = (current_metrics['stockout_rate'] - optimized_metrics['stockout_rate']) / current_metrics['stockout_rate'] * 100
        
        # Calculate ROI
        annual_savings = current_metrics['total_cost'] - optimized_metrics['total_cost']
        implementation_cost = cost_parameters.get('implementation_cost', 10000)
        roi = (annual_savings - implementation_cost) / implementation_cost * 100
        
        return BusinessMetrics(
            cost_reduction=cost_reduction,
            service_level_improvement=service_improvement,
            inventory_turnover=turnover_improvement,
            stockout_reduction=stockout_reduction,
            carrying_cost_savings=annual_savings,
            total_roi=roi
        )
    
    def _calculate_inventory_metrics(self,
                                   inventory_data: pd.DataFrame,
                                   demand_data: pd.DataFrame,
                                   cost_parameters: Dict[str, float]) -> Dict[str, float]:
        """Calculate inventory performance metrics"""
        metrics = {}
        
        # Total inventory value
        total_value = (inventory_data['current_stock'] * inventory_data['unit_cost']).sum()
        
        # Carrying cost
        carrying_cost_rate = cost_parameters.get('carrying_cost_rate', 0.25)
        carrying_cost = total_value * carrying_cost_rate
        
        # Ordering cost (simplified)
        ordering_cost = cost_parameters.get('ordering_cost', 100) * len(inventory_data)
        
        # Total cost
        metrics['total_cost'] = carrying_cost + ordering_cost
        
        # Service level (simplified - assume 95% if stock > safety stock)
        if 'safety_stock' in inventory_data.columns:
            service_level = (inventory_data['current_stock'] >= inventory_data['safety_stock']).mean()
        else:
            service_level = 0.95
        metrics['service_level'] = service_level
        
        # Inventory turnover (simplified)
        if 'annual_demand' in inventory_data.columns:
            avg_inventory = inventory_data['current_stock'].mean()
            total_demand = inventory_data['annual_demand'].sum()
            metrics['inventory_turnover'] = total_demand / avg_inventory if avg_inventory > 0 else 0
        else:
            metrics['inventory_turnover'] = 12  # Default monthly turnover
        
        # Stockout rate (simplified)
        if 'reorder_point' in inventory_data.columns:
            stockout_rate = (inventory_data['current_stock'] < inventory_data['reorder_point']).mean()
        else:
            stockout_rate = 0.05  # Assume 5% stockout rate
        metrics['stockout_rate'] = stockout_rate
        
        return metrics
    
    def generate_performance_report(self,
                                  evaluation_results: BusinessMetrics,
                                  model_comparison: pd.DataFrame) -> str:
        """
        Generate a comprehensive performance report
        
        Args:
            evaluation_results: Business metrics evaluation
            model_comparison: Model comparison results
            
        Returns:
            Formatted performance report string
        """
        report = f"""
# Hospital Inventory AI Module - Performance Report

## Executive Summary
This report presents the performance evaluation of the AI-powered inventory optimization system 
implemented for hospital IT equipment management.

## Business Impact Metrics

### Cost Optimization
- **Total Cost Reduction**: {evaluation_results.cost_reduction:.2f}%
- **Annual Carrying Cost Savings**: ${evaluation_results.carrying_cost_savings:,.2f}
- **Return on Investment**: {evaluation_results.total_roi:.2f}%

### Service Level Improvements
- **Service Level Improvement**: {evaluation_results.service_level_improvement:.2f} percentage points
- **Stockout Reduction**: {evaluation_results.stockout_reduction:.2f}%
- **Inventory Turnover Improvement**: {evaluation_results.inventory_turnover:.2f}x

## Forecasting Model Performance

### Model Comparison Results
{model_comparison.to_string(index=False)}

### Key Findings
- **Best Performing Model**: {model_comparison.iloc[0]['Model']}
- **Lowest MAE**: {model_comparison.iloc[0]['MAE']:.3f}
- **Highest Accuracy**: {model_comparison['Accuracy'].max():.1f}%

## Recommendations

### Immediate Actions
1. **Implement Optimized Inventory Levels**: Deploy the recommended inventory levels for all A-category items
2. **Establish Automated Reordering**: Set up automated reorder triggers based on calculated reorder points
3. **Monitor Performance**: Implement real-time monitoring dashboard for key metrics

### Strategic Improvements
1. **Expand Model Coverage**: Extend forecasting models to include seasonal and promotional effects
2. **Integrate Supply Chain Data**: Incorporate supplier lead time variability into optimization models
3. **Implement Continuous Learning**: Set up automated model retraining based on new data

## Technical Performance

### Model Reliability
- **Forecast Accuracy**: {model_comparison['Accuracy'].mean():.1f}% average across all models
- **Prediction Stability**: {model_comparison['Directional_Accuracy'].mean():.1f}% directional accuracy
- **Error Distribution**: Well-distributed with low bias

### System Integration
- **Database Integration**: Successful connection to hospital PostgreSQL database
- **Real-time Processing**: Capable of processing demand forecasts in real-time
- **Scalability**: Designed to handle increasing data volumes and additional hospitals

## Conclusion

The AI-powered inventory optimization system demonstrates significant potential for improving 
hospital inventory management efficiency. The implemented solution provides:

1. **Quantifiable Cost Savings**: {evaluation_results.cost_reduction:.1f}% reduction in total inventory costs
2. **Improved Service Levels**: Better availability of critical IT equipment
3. **Data-Driven Decision Making**: Automated optimization based on advanced analytics
4. **Scalable Architecture**: Ready for expansion to multiple departments and facilities

## Next Steps

1. **Pilot Implementation**: Start with high-value A-category items
2. **Performance Monitoring**: Track actual vs predicted performance
3. **Continuous Improvement**: Regular model updates and optimization
4. **Stakeholder Training**: Ensure proper system adoption and usage

---
*Report Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report

class ModelValidator:
    """
    Validation utilities for machine learning models
    """
    
    def __init__(self):
        self.validation_results = {}
    
    def cross_validate_forecast(self,
                              model,
                              data: pd.Series,
                              n_splits: int = 5,
                              test_size: int = 30) -> Dict[str, Any]:
        """
        Perform time series cross-validation
        
        Args:
            model: Forecasting model to validate
            data: Time series data
            n_splits: Number of validation splits
            test_size: Size of test set for each split
            
        Returns:
            Dictionary with validation results
        """
        evaluator = ForecastEvaluator()
        results = []
        
        for i in range(n_splits):
            # Calculate split points
            split_point = len(data) - (n_splits - i) * test_size
            
            if split_point <= test_size:
                continue
                
            # Split data
            train_data = data[:split_point]
            test_data = data[split_point:split_point + test_size]
            
            try:
                # Fit model and predict
                model.fit(train_data)
                predictions = model.predict(len(test_data))
                
                # Calculate metrics
                metrics = evaluator.calculate_metrics(test_data.values, predictions)
                results.append({
                    'split': i + 1,
                    'mae': metrics.mae,
                    'rmse': metrics.rmse,
                    'mape': metrics.mape,
                    'accuracy': metrics.accuracy
                })
                
            except Exception as e:
                logger.warning(f"Validation split {i+1} failed: {str(e)}")
                continue
        
        if not results:
            return {'error': 'All validation splits failed'}
        
        # Aggregate results
        results_df = pd.DataFrame(results)
        
        return {
            'mean_mae': results_df['mae'].mean(),
            'std_mae': results_df['mae'].std(),
            'mean_rmse': results_df['rmse'].mean(),
            'std_rmse': results_df['rmse'].std(),
            'mean_accuracy': results_df['accuracy'].mean(),
            'std_accuracy': results_df['accuracy'].std(),
            'detailed_results': results_df
        }
    
    def validate_inventory_optimization(self,
                                      optimizer,
                                      items_data: pd.DataFrame,
                                      demand_forecasts: Dict[str, pd.Series]) -> Dict[str, Any]:
        """
        Validate inventory optimization results
        
        Args:
            optimizer: Inventory optimizer instance
            items_data: Items data
            demand_forecasts: Demand forecasts for items
            
        Returns:
            Validation results
        """
        validation_results = {
            'total_items': len(items_data),
            'successful_optimizations': 0,
            'failed_optimizations': 0,
            'average_cost_reduction': 0,
            'average_service_level': 0,
            'issues': []
        }
        
        cost_reductions = []
        service_levels = []
        
        for _, item in items_data.iterrows():
            item_id = item['item_id']
            
            if item_id not in demand_forecasts:
                validation_results['failed_optimizations'] += 1
                validation_results['issues'].append(f"No forecast available for item {item_id}")
                continue
            
            try:
                # Optimize item
                result = optimizer.optimize_inventory_levels(
                    item.to_dict(),
                    demand_forecasts[item_id]
                )
                
                validation_results['successful_optimizations'] += 1
                service_levels.append(result.service_level)
                
                # Calculate potential cost reduction (simplified)
                current_cost = item.get('current_stock', 0) * item.get('unit_cost', 0) * 0.25
                if current_cost > 0:
                    cost_reduction = (current_cost - result.total_cost) / current_cost * 100
                    cost_reductions.append(cost_reduction)
                
            except Exception as e:
                validation_results['failed_optimizations'] += 1
                validation_results['issues'].append(f"Optimization failed for item {item_id}: {str(e)}")
        
        # Calculate averages
        if cost_reductions:
            validation_results['average_cost_reduction'] = np.mean(cost_reductions)
        if service_levels:
            validation_results['average_service_level'] = np.mean(service_levels)
        
        return validation_results
